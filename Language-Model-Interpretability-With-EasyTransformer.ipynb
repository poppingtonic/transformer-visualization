{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde86f41-02cd-474c-8545-46a7dd5193a0",
   "metadata": {},
   "source": [
    "In this notebook we will explore different language model interpretability methods using a neural networks library written by Neel Nanda (https://github.com/neelnanda-io/Easy-Transformer) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b7527a-47f5-4dbe-a121-185d43567900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc56a65-a7b5-4069-8c93-4789888e11a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plotly_mimetype+notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8600a85-7eff-42b7-b9bc-9c36767767c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader #DataLoada\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4626ca-4683-4610-8a89-f25bb195cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_transformer.utils import (\n",
    "    gelu_new,\n",
    "    to_numpy,\n",
    "    get_corner,\n",
    "    lm_cross_entropy_loss,\n",
    ")# Helper functions\n",
    "from easy_transformer.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")# Hooking utilities\n",
    "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",
    "import easy_transformer\n",
    "from easy_transformer.experiments import (\n",
    "    ExperimentMetric,\n",
    "    AblationConfig,\n",
    "    EasyAblation,\n",
    "    EasyPatching,\n",
    "    PatchingConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838afb5c-4786-49a5-9143-f8c8210b88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308e80f-17a2-4a11-82e3-96c00f04ccb7",
   "metadata": {},
   "source": [
    "## Hook Points\n",
    "In (https://transformer-circuits.pub/2021/garcon/index.html) the good team at Anthropic released a walk through of their internal tool, called Garcon. In Easy-Transformer we, follow in the style of that library by defining a `HookPoint` class. This is a layer to wrap any activation within the model in. The HookPoint acts as an identity function, but allows us to put PyTorch hooks in to edit and access the relevant activation. This allows us to take any model and insert in access points to all interesting activations by wrapping them in HookPoints.\n",
    "\n",
    "There is also a `HookedRootModule` class. This is a utility class that the root module should inherit from (root module = the model we run). It has several utility functions for using hooks as well.\n",
    "\n",
    "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass.\n",
    "\n",
    "The syntax for a hook is `function(activation, hook,)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or _edits the activation_ in-place, that replaces the old one, if it returns None then the activation remains as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bfdec-0f2e-4da7-8f8c-8e861fa63701",
   "metadata": {},
   "source": [
    "### HookPoints Example\n",
    "Here's a simple example of how to use the classes:\n",
    "\n",
    "We define a basic network with two layers that each take a scalar input x, square it, and add a constant: $x_0=x, x_1=x_0{^2}+3,x_2=x_1{^2}-4$.\n",
    "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa369e9f-944c-41dc-b8cf-6dc21c8888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_transformer.hook_points import HookedRootModule, HookPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d390b720-8407-4abe-9c25-9f0940ae4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareAdd(nn.Module):\n",
    "    def __init__(self, offset):\n",
    "        super().__init__()\n",
    "        self.offset = nn.Parameter(torch.tensor(offset))\n",
    "        self.hook_square = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The hook_square doesn't change the value, but lets us access it\n",
    "        square = self.hook_square(x * x)\n",
    "        return self.offset + square\n",
    "    \n",
    "\n",
    "class TwoLayerModel(HookedRootModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = SquareAdd(3.0)\n",
    "        self.layer2 = SquareAdd(-4.0)\n",
    "        self.hook_in = HookPoint()\n",
    "        self.hook_mid = HookPoint()\n",
    "        self.hook_out = HookPoint()\n",
    "        \n",
    "        # We need to call the setup function of HookedRootModule  to build an\n",
    "        # internal dictionary of modules and hooks, and to give each hook a name.\n",
    "        super().setup()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # We wrap the input and each layer's output in a hook - they leave the\n",
    "        # value unchanged (unless there's a hook added to explicitly change it),\n",
    "        # but allow us to access it.\n",
    "        x_in = self.hook_in(x)\n",
    "        x_mid = self.hook_mid(self.layer1(x_in))\n",
    "        x_out = self.hook_out(self.layer2(x_mid))\n",
    "        return x_out\n",
    "\n",
    "model = TwoLayerModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8f11d-e8e5-4f63-afb1-7e7166ca9802",
   "metadata": {},
   "source": [
    "We can add a cache, to save the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
