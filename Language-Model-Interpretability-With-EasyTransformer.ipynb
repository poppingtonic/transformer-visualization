{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde86f41-02cd-474c-8545-46a7dd5193a0",
   "metadata": {},
   "source": [
    "In this notebook we will explore different language model interpretability methods using a neural networks library written by Neel Nanda (https://github.com/neelnanda-io/Easy-Transformer) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b7527a-47f5-4dbe-a121-185d43567900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc8a2be-7fdf-4f2b-8e45-2ed6b17d4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: transformers in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (4.23.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: filelock in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.11.0 tenacity-8.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61365fb-a85a-4d3b-9337-4f8858b5b7f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neelnanda-io/Easy-Transformer.git\n",
      "  Cloning https://github.com/neelnanda-io/Easy-Transformer.git to /tmp/pip-req-build-lv_xkrqb\n",
      "  Running command git clone -q https://github.com/neelnanda-io/Easy-Transformer.git /tmp/pip-req-build-lv_xkrqb\n",
      "  Resolved https://github.com/neelnanda-io/Easy-Transformer.git to commit 57411fcfe9592803eb20970639b979f22cb6cf9f\n",
      "Requirement already satisfied: einops in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (0.3.2)\n",
      "Requirement already satisfied: numpy in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.21.2)\n",
      "Requirement already satisfied: torch in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.11.0)\n",
      "Requirement already satisfied: datasets in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.15.1)\n",
      "Requirement already satisfied: transformers in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (4.23.1)\n",
      "Requirement already satisfied: tqdm in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: wandb in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (0.13.2)\n",
      "Collecting fancy_einsum\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting torchtyping\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: rich in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from easy-transformer==0.1.0) (11.2.0)\n",
      "Requirement already satisfied: packaging in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (21.3)\n",
      "Requirement already satisfied: aiohttp in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (2.25.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: xxhash in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (2022.11.0)\n",
      "Requirement already satisfied: dill in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from datasets->easy-transformer==0.1.0) (0.3.5.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from aiohttp->datasets->easy-transformer==0.1.0) (1.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->easy-transformer==0.1.0) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->easy-transformer==0.1.0) (6.0)\n",
      "Requirement already satisfied: filelock in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->easy-transformer==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from packaging->datasets->easy-transformer==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from requests>=2.19.0->datasets->easy-transformer==0.1.0) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from pandas->easy-transformer==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from pandas->easy-transformer==0.1.0) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->easy-transformer==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from rich->easy-transformer==0.1.0) (2.11.2)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from rich->easy-transformer==0.1.0) (0.4.5)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from rich->easy-transformer==0.1.0) (0.9.1)\n",
      "Collecting typeguard>=2.11.1\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers->easy-transformer==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from transformers->easy-transformer==0.1.0) (2022.6.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.6.0)\n",
      "Requirement already satisfied: setproctitle in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (5.9.2)\n",
      "Requirement already satisfied: pathtools in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (3.1.18)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (58.0.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (8.0.4)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from wandb->easy-transformer==0.1.0) (3.20.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb->easy-transformer==0.1.0) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/mu/miniconda3/envs/rave/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->easy-transformer==0.1.0) (5.0.0)\n",
      "Building wheels for collected packages: easy-transformer\n",
      "  Building wheel for easy-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easy-transformer: filename=easy_transformer-0.1.0-py3-none-any.whl size=65986 sha256=6c26b1de10710ba3397402c7a7eb9a4f9d8a44d016c86a536287b4eb60f0f143\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fnpteubg/wheels/1e/c9/25/990e0b8fb9b1e4e6d167878f6e24a13c74ef322d7d2d2e2dd5\n",
      "Successfully built easy-transformer\n",
      "Installing collected packages: typeguard, torchtyping, fancy-einsum, easy-transformer\n",
      "Successfully installed easy-transformer-0.1.0 fancy-einsum-0.0.3 torchtyping-0.1.4 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install \"git+https://github.com/neelnanda-io/Easy-Transformer.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc56a65-a7b5-4069-8c93-4789888e11a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plotly_mimetype+notebook'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8600a85-7eff-42b7-b9bc-9c36767767c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader #DataLoada\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4626ca-4683-4610-8a89-f25bb195cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_transformer.utils import (\n",
    "    gelu_new,\n",
    "    to_numpy,\n",
    "    get_corner,\n",
    "    lm_cross_entropy_loss,\n",
    ")# Helper functions\n",
    "from easy_transformer.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")# Hooking utilities\n",
    "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",
    "import easy_transformer\n",
    "from easy_transformer.experiments import (\n",
    "    ExperimentMetric,\n",
    "    AblationConfig,\n",
    "    EasyAblation,\n",
    "    EasyPatching,\n",
    "    PatchingConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838afb5c-4786-49a5-9143-f8c8210b88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308e80f-17a2-4a11-82e3-96c00f04ccb7",
   "metadata": {},
   "source": [
    "## Hook Points\n",
    "In (https://transformer-circuits.pub/2021/garcon/index.html) the good team at Anthropic released a walk through of their internal tool, called Garcon. In Easy-Transformer we, follow in the style of that library by defining a `HookPoint` class. This is a layer to wrap any activation within the model in. The HookPoint acts as an identity function, but allows us to put PyTorch hooks in to edit and access the relevant activation. This allows us to take any model and insert in access points to all interesting activations by wrapping them in HookPoints.\n",
    "\n",
    "There is also a `HookedRootModule` class. This is a utility class that the root module should inherit from (root module = the model we run). It has several utility functions for using hooks as well.\n",
    "\n",
    "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass.\n",
    "\n",
    "The syntax for a hook is `function(activation, hook,)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or _edits the activation_ in-place, that replaces the old one, if it returns None then the activation remains as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bfdec-0f2e-4da7-8f8c-8e861fa63701",
   "metadata": {},
   "source": [
    "### HookPoints Example\n",
    "Here's a simple example of how to use the classes:\n",
    "\n",
    "We define a basic network with two layers that each take a scalar input x, square it, and add a constant: $x_0=x, x_1=x_0{^2}+3,x_2=x_1{^2}-4$.\n",
    "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa369e9f-944c-41dc-b8cf-6dc21c8888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easy_transformer.hook_points import HookedRootModule, HookPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d390b720-8407-4abe-9c25-9f0940ae4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareAdd(nn.Module):\n",
    "    def __init__(self, offset):\n",
    "        super().__init__()\n",
    "        self.offset = nn.Parameter(torch.tensor(offset))\n",
    "        self.hook_square = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The hook_square doesn't change the value, but lets us access it\n",
    "        square = self.hook_square(x * x)\n",
    "        return self.offset + square\n",
    "    \n",
    "\n",
    "class TwoLayerModel(HookedRootModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = SquareAdd(3.0)\n",
    "        self.layer2 = SquareAdd(-4.0)\n",
    "        self.hook_in = HookPoint()\n",
    "        self.hook_mid = HookPoint()\n",
    "        self.hook_out = HookPoint()\n",
    "        \n",
    "        # We need to call the setup function of HookedRootModule  to build an\n",
    "        # internal dictionary of modules and hooks, and to give each hook a name.\n",
    "        super().setup()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # We wrap the input and each layer's output in a hook - they leave the\n",
    "        # value unchanged (unless there's a hook added to explicitly change it),\n",
    "        # but allow us to access it.\n",
    "        x_in = self.hook_in(x)\n",
    "        x_mid = self.hook_mid(self.layer1(x_in))\n",
    "        x_out = self.hook_out(self.layer2(x_mid))\n",
    "        return x_out\n",
    "\n",
    "model = TwoLayerModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8f11d-e8e5-4f63-afb1-7e7166ca9802",
   "metadata": {},
   "source": [
    "We can add a cache, to save the activation at each hook point.\n",
    "\n",
    "There's a custom `run_with_cache` function on the root module as a convenience, which is a wrapper around `model.forward` that returns `model_out`, `cache_object`, we could also manually add hooks with `run_with_hooks` that store activations in a global caching dictionary. This is often useful if we only want to store e.g. subsets or functions of some activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbda008-8493-4266-89d3-e12daeb7d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: 780.0\n",
      "Value cached at hook hook_in 5.0\n",
      "Value cached at hook layer1.hook_square 25.0\n",
      "Value cached at hook hook_mid 28.0\n",
      "Value cached at hook layer2.hook_square 784.0\n",
      "Value cached at hook hook_out 780.0\n"
     ]
    }
   ],
   "source": [
    "out, cache = model.run_with_cache(torch.tensor(5.0))\n",
    "print(\"Model output:\", out.item())\n",
    "for key in cache:\n",
    "    print(f\"Value cached at hook {key}\", cache[key].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082ccd4-70e4-4469-a8d5-4c9332aa4925",
   "metadata": {},
   "source": [
    "We can also use hooks to intervene on activations: e.g. we can set the intermediate value in layer 2 to zero to change the output to -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c09da5-f2dc-411a-8a2a-f94af899f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2.hook_square\n",
      "Output after intervening on layer2.hook_square -4.0\n"
     ]
    }
   ],
   "source": [
    "def set_to_zero_hook(tensor, hook):\n",
    "    print(hook.name)\n",
    "    return torch.tensor(0.0)\n",
    "\n",
    "print(\n",
    "    \"Output after intervening on layer2.hook_square\",\n",
    "    model.run_with_hooks(\n",
    "        torch.tensor(5.0), fwd_hooks=[(\"layer2.hook_square\", set_to_zero_hook)]\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7236ded-835a-479d-bc74-e0c892c124e1",
   "metadata": {},
   "source": [
    "## Transformer models\n",
    "We now define a stripped-down transformer. There are helper functions to load in the weights of several families of open-source LLMs - OpenAI's GPT-2, Facebook's OPT and Eleuther's GPT-Neo.\n",
    "\n",
    "Note: OPT-350M is not supprted - it applies the LayerNorms to the _outputs_ of each layer, which means we cannot fold the weights and biases into other layers, and would require notably different architecture.\n",
    "\n",
    "The list of supported model names:\n",
    "```\n",
    "[\n",
    "    'gpt2-small',\n",
    "    'gpt2-medium',\n",
    "    'gpt2-xl',\n",
    "    'facebook/opt-125m',\n",
    "    'facebook/opt-1.3b',\n",
    "    'facebook/opt-2.7b',\n",
    "    'facebook/opt-6.7b',\n",
    "    'facebook/opt-13b',\n",
    "    'facebook/opt-30b',\n",
    "    'facebook/opt-66b',\n",
    "    'EleutherAI/gpt-neo-125M'\n",
    "    'EleutherAI/gpt-neo-1.3B',\n",
    "    'EleutherAI/gpt-neo-2.7B'\n",
    "]\n",
    "```\n",
    "\n",
    "### Examples\n",
    "#### Setup\n",
    "Load in GPT2-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0323e4ed-cd5a-4a18-bd3b-94b05b23e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23bfee415634c1f8602b734e2e87ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2d1f2a86d1440d93c9feb81846062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67d95ef6f9c407c9f416bacec4b6698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494247813f384b4fabe04599d638419a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f3d2c105bd4d9aa1f8b9c0b61b2b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = EasyTransformer.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3ed31-366b-429e-aaa3-a047cf88424a",
   "metadata": {},
   "source": [
    "Create some reference text to run the models on. Models come with a `to_tokens` and `to_str_tokens` method, which can convert texst to tokens and to a list of individual tokens as strings. Though GPT-2 was not trained with a beginning of string token, we prepend one by default, as the first token is often used as a \"resting position\" by inactive attention heads, and as a result has weird behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a699ea38-e3f8-44e8-9a01-58c4fa11416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'The', ' rain', ' in', ' Spain', ' stays', ' mainly', ' on', ' the', ' plain']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The rain in Spain stays mainly on the plain\"\n",
    "# The model has a method to_str_tokens\n",
    "print(model.to_str_tokens(prompt, prepend_bos=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e605cb2-96b3-4acf-913f-bc818da31491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'After', ' the', ' lunch', ',', ' Mary', ' and', ' Bob', ' went', ' to', ' the', ' beach', '.', ' Bob', ' gave', ' a', ' candle', ' to', ' Mary', '.']\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"After the lunch, Mary and Bob went to the beach. Bob gave a candle to Mary.\"\n",
    "tokens_2 = model.to_tokens(prompt_2, prepend_bos=True)\n",
    "# to_str_tokens also takes a tensor of tokens as input, though only for a *single* example\n",
    "print(model.to_str_tokens(tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5155289-cd44-4292-9cbf-5925221fd198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: Hyperparameters for the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_layers': 12,\n",
       " 'd_model': 768,\n",
       " 'n_ctx': 1024,\n",
       " 'd_head': 64,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_heads': 12,\n",
       " 'd_mlp': 3072,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'd_vocab': 50257,\n",
       " 'eps': 1e-05,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_local_attn': False,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'from_checkpoint': False,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'window_size': None,\n",
       " 'attn_types': None,\n",
       " 'init_mode': 'gpt2',\n",
       " 'normalization_type': 'LNPre',\n",
       " 'device': 'cuda',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'seed': 42,\n",
       " 'initializer_range': 0.02886751345948129,\n",
       " 'init_weights': False,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'final_rms': False,\n",
       " 'd_vocab_out': 50257,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'rotary_dim': None,\n",
       " 'n_params': 84934656}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reference: Hyperparameters for the model\")\n",
    "dataclasses.asdict(model.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d9701-d953-4a40-b39b-5a2ae6ce62ce",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b32a15-36d4-4e51-af65-601d672d7214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
